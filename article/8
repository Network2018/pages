<!-- build time:Tue May 01 2018 19:40:15 GMT+0800 (中国标准时间) --><!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><link href="https://static.sometimesnaive.org:9004/deps/imgs/favicon.ico" type="image/x-icon" rel="icon"><title>也谈TCP拥塞控制技术 与BBR的加速原理 | 南琴浪博客</title><link href="https://static.sometimesnaive.org:9004/deps/css/style.min.css" type="text/css" rel="stylesheet"><script src="https://static.sometimesnaive.org:9004/deps/js/jquery.min.js" type="application/javascript"></script><link href="https://static.sometimesnaive.org:9004/deps/css/nprogress.min.css" type="text/css" rel="stylesheet"><script src="https://static.sometimesnaive.org:9004/deps/js/nprogress.min.js" type="application/javascript"></script><link href="https://static.sometimesnaive.org:9004/deps/css/prism.min.css" type="text/css" rel="stylesheet"><script src="https://static.sometimesnaive.org:9004/deps/js/prism.min.js" type="application/javascript"></script><link href="https://static.sometimesnaive.org:9004/deps/css/gitment.min.css" type="text/css" rel="stylesheet"><script src="https://static.sometimesnaive.org:9004/deps/js/gitment.min.js" type="application/javascript"></script></head><body><header class="header fixed-header"><div class="header-container"><a class="home-link" href="/">南琴浪博客</a><ul class="right-list"><li class="list-item"><a href="/" class="item-link">Home</a></li><li class="list-item"><a href="/archive/" class="item-link">Archive</a></li><li class="list-item"><a href="/portal/feed" class="item-link">RSS</a></li><li class="list-item"><a href="/portal/friendly-links" class="item-link">Links</a></li></ul><div class="menu-mask"><ul class="menu-list"><li class="menu-item"><a href="/" class="menu-link">Home</a></li><li class="menu-item"><a href="/archive/" class="menu-link">Archive</a></li><li class="menu-item"><a href="/portal/feed" class="menu-link">RSS</a></li><li class="menu-item"><a href="/portal/friendly-links" class="menu-link">Links</a></li></ul></div></div></header><div id="article-banner"><h2>也谈TCP拥塞控制技术 与BBR的加速原理</h2><p class="post-date">10/04/2017</p></div><main class="app-body flex-box"><article class="post-article"><section class="markdown-content"><p>本文会简要的讲一下关于 <code>TCP 拥塞控制技术</code> 的由来与发展，以及现在很常见的 BBR 的加速原理<br><a id="more"></a></p><h2 id="什么是拥塞"><a href="#什么是拥塞" class="headerlink" title="什么是拥塞"></a>什么是拥塞</h2><p>拥塞现象，是指数据到达通信子网的过程中，某一部分的分组数量过多，使得该部分网络来不及处理，以致引起这部分乃至整个网络性能下降的现象。严重时会导致网络陷入死锁。<br>这种现象好比<code>公路上常见的交通拥挤</code>，当节假日公路车辆大量增加时，道路拥堵，使每辆车到达目的地的时间增加，即发生了拥塞。</p><h2 id="什么是拥塞控制技术"><a href="#什么是拥塞控制技术" class="headerlink" title="什么是拥塞控制技术"></a>什么是拥塞控制技术</h2><p>拥塞控制（Tcp Congestion Control），就是针对此问题的控制技术（应对方案），但也不能说是应对，拥塞控制只能尽量缓解拥塞。<br>拥塞控制用于调整 TCP 连接单次发送的分组数量（单次发送量，在英文和代码中称做 cwnd），它通过逐步调整单次发送量，使其逼近当前网络的承载量。<br>拥塞控制的目标是<code>最大限度地利用网络带宽，同时不产生数据流传输中的拥塞现象</code>。<br>这也是 锐速 和 BBR 能起到明显加速效果的原因。</p><h2 id="传统的TCP拥塞控制算法"><a href="#传统的TCP拥塞控制算法" class="headerlink" title="传统的TCP拥塞控制算法"></a>传统的TCP拥塞控制算法</h2><p><code>传统的 TCP 拥塞控制</code>主要是基于 Van Jacobson 博士提出的<code>慢启动</code>算法、<code>拥塞避免</code>算法和用于<code>估计周转 RTT(Round Trip Time)</code>的算法。</p><p>慢启动算法通过观察<code>进入网络的速率应该与另一端返回确认的速率相同</code>进行工作。慢启动为发送方增加了一个称为“拥塞窗口”（英文中称为 cwnd ）的窗口，当与接收方建立 TCP 连接时，拥塞窗口被初始化为 1 个报文段（即另一端通告的报文段大小）。接收方每收到一个 ACK ，拥塞窗口就增加 1 个报文段（ cwnd 以字节为单位，而慢启动以报文段大小为单位进行增加），发送方取拥塞窗口与通告窗口中的最小值作为发送上限，即<code>拥塞窗口是发送方的流量控制，通告窗口是接收方的流量控制</code>。发送方开始时发送一个报文段，然后等待 ACK ，收到 ACK 后拥塞窗口从 1 增加到 2 ，即可以发送两个报文段。当收到这两个报文段的 ACK 后拥塞窗口则增加为 4 ，拥塞窗口呈指数式增长。</p><p>然后，在某些传输段可能会达到容量上限，中间路由便开始丢弃分组。拥塞避免算法就是用于处理丢失分组的算法。<br>拥塞避免算法假定分组丢失是非常少的（远小于 1% ），因此认为分组丢失就意味着<code>在发送方和接收方之间的某个传输段上发生了拥塞</code>。<br>慢启动和拥塞避免算法是两个目的不同、独立的算法，但在实际中这两个算法通常在一起实现。当拥塞发生时，我们希望降低分组进入拥堵段的速率，于是设计了慢启动方式来做到这一点。</p><p>1990 年出现的 TCP Reno 版本，增加了<code>快速重传</code>算法和<code>快速恢复</code>算法，避免了当网络拥塞不够严重却因采用慢启动算法而造成过大地减小发送窗口尺寸的现象。</p><p>1.<strong>拥塞控制的四个阶段</strong></p><ol><li>慢启动阶段(Slow Start)：发送方一开始便向网络发送多个报文段，直至达到接收方的通告窗口大小为止。当发送方和接收方处于同一局域网时这种方式值得采用。但假如在发送方和接收方之间存在多个路由或速率较低的链路时，就可能引起一些问题。某些中间路由必须缓存分组，并有可能耗尽存储空间。</li><li>拥塞避免阶段(Congestion Avoidance)：当发现有超时或收到 3 个相同 ACK 确认帧时，认为有丢包发生，此时网络已发生拥塞现象，需进行相应的拥塞控制：将慢启动阈值设置为当前拥塞窗口的一半；若检测到超时，拥塞窗口就被置为 1 。假如拥塞窗口 &lt;= 慢启动阈值，则重新进入慢启动阶段；假如拥塞窗口大于慢启动阈值，执行拥塞避免算法。</li><li>快速重传阶段(Fast Retransmit)：当发送方收到三个相同的 ACK 副本时，认为有丢包发生，并迅速<code>令发送方重传丢失的数据包</code>，而不必等待 RTO 超时。同时将 ssthresh 设置为当前 cwnd 值的一半，并且将 cwnd 减为原先的一半。</li><li>快速恢复阶段(Fast Recovery)：当旧数据包离开网络后，才能发送新数据包进入网络，即<code>同一时刻在网络中传输的数据包数量是恒定的</code>。假如发送方收到一个重复的 ACK ，则认为已经有一个数据包离开了链路，并将拥塞窗口加 1 。</li></ol><p>2.<strong>对传统TCP拥塞控制机制的发展及改进</strong></p><ol><li>对慢启动的改进<br><code>慢启动算法通过逐渐增加 cwnd 大小来探测可用的网络容量，防止连接一开始便采用过大发送量导致网络拥塞</code>。然而有时该算法也会浪费可用的网络容量，因为慢启动算法总是从 cwnd = 1 开始，每收到一个 ACK ，cwnd 增加 1 ，对于高 RTT 网络，为使 cwnd 达到合适值，需要花费很长时间在探测上。因此可设置较大初始窗口，大的初始窗口避免了延迟 ACK 机制单个报文段初始窗口的等待超时问题，缩短了小 TCP 流的传输时间和大延迟链路上的慢启动时间。</li><li>对重传与恢复的改进<br>为了避免不必要的重传超时，有人提出了一种受限传输机制：发送方接收到一个或两个重复 ACK 后，才继续传输新的数据报文段。从而避免了不必要的重传。<br>有很多情况下，数据报文段并没有丢失，但发送方可能会误判数据报文段丢失，然后调用拥塞控制减小拥塞窗口大小。比如当重传定时过早溢出时，发送方在重传数据报文段时不必要地减小了拥塞窗口，而这时实际上并没有数据报文段丢失。假如是由于数据报文段的重新组织而不是数据报文段的丢失，导致 3 个重复确认，同样会导致发送方不必要地在快速重传数据报文段后减小拥塞窗口。<br>假如发送方在重传数据报文段一个 RTT 后发现接收方接收到了重传数据报文段的两个拷贝，则可以推断重传是不必要的。这时，发送方可以撤销对拥塞窗口的减小。发送方可以通过将慢启动阀值增加到初始值，使拥塞窗口恢复至初始值。除了恢复拥塞窗口，发送方还可以调整重复确认阀值或者重传超时参数来避免多次不必要的重传。</li><li>对公平性的改进<br>在拥塞避免阶段，假如没有发生丢包事件，则发送方的 cwnd 在每个 RTT 时间内大约可以增加一个报文段大小，但这样会造成具有不同 RTT 或窗口尺寸的多个连接在瓶颈处对带宽竞争的不公平性。大 RTT 或小窗口的连接，相应的 cwnd 增长速度也相对缓慢，所以只能得到很少一部分带宽。<br>要解决上述问题，可以通过在路由处使用公平队列和 TCP 友好缓存来进行控制以增加公平性。假如没有路由的参与，要增加公平性，就要求发送方的拥塞控制进行相应的改变，在拥塞避免阶段使共享同一资源的各个 TCP 连接以相同速度发送数据，从而确保了各个连接间的公平性。</li></ol><h2 id="新的拥塞控制技术-BBR"><a href="#新的拥塞控制技术-BBR" class="headerlink" title="新的拥塞控制技术 - BBR"></a>新的拥塞控制技术 - BBR</h2><p>BBR 是谷歌内部员工开发的新 TCP 拥塞控制技术，和锐速一样属于激进算法<br>上文提到过，锐速和 BBR 加速的作用在于最大限度利用网络带宽，同时不产生数据流传输中的拥塞现象<br>但同时这也引起了 TCP 连接的公平性问题和可能的多余窗口发送</p><h3 id="BBR-的组成"><a href="#BBR-的组成" class="headerlink" title="BBR 的组成"></a>BBR 的组成</h3><p>概括来讲，BBR 由 5 部分组成：</p><ol><li>即时速率的计算<br>计算一个即时的带宽，该带宽值是 BBR 后续一切计算的基准。BBR 会根据当前的即时带宽以及其所处的 pipe 状态来计算 pacing rate 以及 cwnd ，后面我会提到，<code>这个即时带宽计算方法的突破式改进是 BBR 简单且高效的根源</code>，计算方案按照数学计算，不再局限于数据的含义。在 BBR 运行过程中，系统会跟踪当前时段的最大即时带宽。</li><li>RTT 的跟踪<br>BBR 之所以可以获取高带宽利用率，是因为它可以<code>非常奔放地探测到带宽的最大值以及 RTT 的最小值</code>，这样计算出来的 BDP 就是目前为止 TCP 管道的最大容量。BBR 的目标就是充分利用这个最大容量，这个目标最终驱动了 cwnd 的计算。在 BBR 运行过程中，系统会跟踪当前时段最小 RTT 。</li><li>pipe 状态机<br>BBR 根据 TCP 的拥塞状况，有针对性地定义了 4 种状态，即 <code>STARTUP</code> ，<code>DRAIN</code> ，<code>PROBE_BW</code> ，<code>PROBE_RTT</code> 这四种状态。BBR 通过<code>对上述计算的即时带宽 BW 和 RTT 的持续观察，在这四种状态之间自由切换</code>，相比以前的所有拥塞控制算法，其革命性的改进在于 BBR 不再跟踪系统的 TCP 拥塞状态机，而旨在用统一且自由的方式来应对 pacing rate 和 cwnd 的计算，不管当前 TCP 是处在 Open 还是处在 Disorder 状态，抑或已到达 Recovery 状态。事实就是，<code>BBR 忽略丢包的存在，只考虑对 BW 和 RTT 的观察</code>。</li><li>输出：pacing rate 和 cwnd<br>BBR 的输出并不仅仅是一个 cwnd ，更重要的是 pacing rate 。在传统算法中，cwnd 是 TCP 拥塞控制算法的唯一输出，但它仅仅规定了当前 TCP 最多可以发送多少数据流，它并没有规定怎么把这么多数据发送出去，在 Linux 中，如果发出这么多数据呢？简单而粗暴 - <code>突发</code>！忽略接收端通告窗口的前提下，Linux 会把 cwnd 窗口数据全部突发出去，而这往往会造成路由的排队。BBR 在计算 cwnd 的同时，还计算了一个与之适配的 pacing rate ，该 pacing rate 规定 cwnd 指示的窗口数据的数据包之间以多大的时间间隔发送出去。</li><li>外部机制的利用<br>BBR 可以高效地运行且如此简单，还在于<code>很多机制并不是它本身实现的，而是利用了外部的已有机制</code>。<br>下文中将阐述它为什么在计算带宽 BW 时能做到如此奔放地将重传数据也计算在内。</li></ol><h3 id="带宽的计算"><a href="#带宽的计算" class="headerlink" title="带宽的计算"></a>带宽的计算</h3><h4 id="即时带宽-BW-的计算"><a href="#即时带宽-BW-的计算" class="headerlink" title="即时带宽 BW 的计算"></a>即时带宽 BW 的计算</h4><p>BBR <code>完全忽略系统层面的 TCP 状态</code>，计算带宽时它仅需要两个值：</p><ul><li>应答了多少数据，记为 delivered</li><li>应答 delivered 所用的时间，记为 interval_us</li></ul><p>将上述二者相除，就能得到带宽：<br>BW = delivered/interval_us</p><p>以上的计算完全是标量计算，只关注数据的大小，不关注数据的含义。例如 delivered 的采集中，BBR 根本不关心某个应答是重传后的 ACK 确认的、正常 ACK 确认的、还是 SACK 确认的。<code>BBR 只关心被应答了多少</code>。<br>这和 TCP/IP 网络模型是一致的，因为在中间链路上，路由交换机也不会去管这些数据包是重传的还是乱序的，然而拥塞也是在这些地方发生的，既然拥塞点都不关心数据的意义，TCP 为什么要关注呢？拥塞发生的原因，即数据量超过了路由带宽限制，利用这一点，<code>只需要精心地控制发送数据量</code>就好了，完全不用管什么乱序、重传之类的。</p><h4 id="pacing-rate-以及-cwnd-的计算"><a href="#pacing-rate-以及-cwnd-的计算" class="headerlink" title="pacing rate 以及 cwnd 的计算"></a>pacing rate 以及 cwnd 的计算</h4><p>pacing rate 怎么计算？就是即时带宽 BW 。而即时带宽 BW 是上一次采样计算的结论，这次能否按照这个 BW 发送数据呢？这要看当前的增益系数的值，设为 G ，那么 G×BW 就是 pacing rate 的值。</p><p>至于 cwnd 的计算，cwnd 其实描述了一条网络管道（而 rwnd 描述了接收端缓冲区），因此 cwnd 其实就是这个管道的容量即 BDP 。BW 已经有了，还缺少 RTT 。BBR 一直在持续搜集最小 RTT ，BBR 并没有采用类似移动指数平均算法来猜测 RTT ，而是直接采集最小 RTT（这个 RTT 是 TCP 系统层面移动指数平均的结果，即 SRTT ，但 BBR 并不会对此结果再次做平均。最小 RTT 表示一个曾经达到的最佳 RTT ，既然曾经达到过，说明可以再次达到该 RTT ，这样有益于网络管道利用率最大化。通过 G’×BDP 就算出了 cwnd ，这里的 G’是 cwnd 的增益系数，与带宽增益系数 G 含义一样，根据 BBR 的状态机获取。</p><h4 id="状态机"><a href="#状态机" class="headerlink" title="状态机"></a>状态机</h4><p>不断地基于当前带宽以及当前的增益系数计算 pacing rate 以及 cwnd ，以这 2 个结果作为拥塞控制算法的输出。在 TCP 连接的持续过程中，每收到一个 ACK ，都会计算即时带宽，然后将结果反馈给 BBR 的 pipe 状态机，不断地调节增益系数。<br>可以发现，它是一个典型的封闭反馈系统，与 TCP 当前处于什么拥塞状态完全无关。这非常不同于之前的所有拥塞控制算法，在之前的算法中，算法内部是受外部的拥塞状态影响的，比方说在 Recovery 状态下，甚至都不会进入拥塞控制算法。<br>在 BBR 问世前，Linux 使用 PRR 算法控制 Recovery 状态的窗口调整，即使这个时候网络已经恢复，TCP 也无法发现， 因为 TCP 的 Recovery 状态还未恢复到 Open 。</p></section><br><div class="article-footer"><br><p>本文可能过时失效，若需更新，请留言</p><p>本博客文章均为原创，转载请注明来源</p></div><div class="nav-container"><a class="nav-right" href="/article/7">Linux 的默认 SSH 登录路径 <span class="nav-arrow">→</span> </a><a class="nav-left" href="/article/9"><span class="nav-arrow">← </span>常用的访问安全 HTTP 响应头</a></div><div id="comments"></div></article><aside class="catalog-container"><div class="toc-main"><strong class="toc-title">Catalog</strong><ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#什么是拥塞"><span class="toc-nav-text">什么是拥塞</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#什么是拥塞控制技术"><span class="toc-nav-text">什么是拥塞控制技术</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#传统的TCP拥塞控制算法"><span class="toc-nav-text">传统的TCP拥塞控制算法</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#新的拥塞控制技术-BBR"><span class="toc-nav-text">新的拥塞控制技术 - BBR</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#BBR-的组成"><span class="toc-nav-text">BBR 的组成</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#带宽的计算"><span class="toc-nav-text">带宽的计算</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#即时带宽-BW-的计算"><span class="toc-nav-text">即时带宽 BW 的计算</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#pacing-rate-以及-cwnd-的计算"><span class="toc-nav-text">pacing rate 以及 cwnd 的计算</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#状态机"><span class="toc-nav-text">状态机</span></a></li></ol></li></ol></li></ol></div></aside></main><script>!function(){var e="https://static.sometimesnaive.org:9004/deps/imgs/banner.jpg";$("#article-banner").css({"background-image":"url("+e+")"}),$(".header").removeClass("fixed-header")}()</script><script>(function(){
    var gitmentConfig = "nanqinlang"
    if (gitmentConfig !== 'undefined') {
        var gitment = new Gitment({
            id: "8",
            owner: "nanqinlang",
            repo: "sometimesnaive.org",
            oauth: {client_id: "b0612de512484ab34c7a", client_secret: "c2bc4cd3c947097d2ccc4c16f120d50458314dd0"},
            theme: {render(state, instance){const container = document.createElement('div');container.lang="en-US";container.className='gitment-container gitment-root-container';container.appendChild(instance.renderHeader(state, instance));container.appendChild(instance.renderEditor(state, instance));container.appendChild(instance.renderComments(state, instance));container.appendChild(instance.renderFooter(state, instance));return container;}}
        })
        gitment.render(document.getElementById('comments'))
    }
})();</script><div class="scroll-top"><span class="arrow-icon"></span></div><footer class="app-footer"><p class="copyright" align="center">&copy; 2017 - 2018 南琴浪博客 <a>(sometimesnaive.org)<a></a></a></p></footer><script src="https://static.sometimesnaive.org:9004/deps/js/function.min.js" type="application/javascript"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108266909-1","auto"),ga("send","pageview")</script><script async src="https://www.google-analytics.com/analytics.js" type="application/javascript"></script></body></html><!-- rebuild by neat -->